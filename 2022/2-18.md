# February 18, 2022

## Logistics

12-1 PM EST / 9-10AM PST / 5-6PM GMT

## Agenda

* Call for Consensus: Charter (10 mins)
* Fraud Prevention Trusted Server (15 mins)
* Current Proposals (15 mins)

## Resources

* [Minutes](https://docs.google.com/document/d/1qzvH8s5eqFAoQ5ffK6KKTve-zYpJeU4ZOjPYWk3GeVM/edit?usp=sharing)
* [Chair Slides](https://docs.google.com/presentation/d/1oYINb0cqy1PJ1oW4U8HsBeo7Hmn2NbxP4lBsc10cKQM/view)
* [Fraud Prevention Trusted Server](https://docs.google.com/presentation/d/1AtYUKpgSlH3d6ELPhlGcy29IojGk94GZYOXRO78-JtE/view)

## Minutes

### Charter

Steven Valdez: consensus call for the charter based on feedback. Thread on the public-antifraud list. Call slated to end by March 1st. Explicit support, feedback and comments all solicited. Scope of work expanded to be clear that we include different properties of fraud, and abuse wording that was added previously.

Per: whether this CG should consider only technical features, but also non-technical measures, like auditing or compliance.

Steven: we can’t really make policy or put constraints on others, but could incubate a document for development elsewhere.

Per: document the needs for non-technical solutions, even if we don’t detail/solve those.

Sam Jackson: Certification requirements?

Per: +1, just note/identify when there are some non-technical requirements even if we can’t solve it.

Brad Lassey: nothing about Community Groups that prevents us from issuing reports on non-technical measures. Just a charter question for us of whether we want it to be in our scope or not.

Sam: interest from FIDO Alliance folks, who could help with framework of certification. https://fidoalliance.org/

Per: we should think holistically how to solve, and consider in technical measures which will rely on non-technical as well.

(Ian off voice: One proposal is to wait until the need arises, then update the charter accordingly.)

Nick: charter sections talk about voting, don’t think it’ll be particularly effective here. Not sure if we should object?

Steven: Voting is there as failsafe if we can’t get through consensus and electing chairs. I think we need some sort of fallback when can’t establish consensus or if there are no chairs. But it’s not the primary means for anything.

(Ian off voice: The CG Charter template includes these vote provisions for chair selection https://w3c.github.io/cg-charter/CGCharter.html ; so I don’t think this is an unusual charter in that regard.)

Nick: I do think it’s useful to emphasize as a fallback. I don’t think this is standard as a decision process for multistakeholder consensus groups.

Steven: I can try pointing more things at the decision process

Per: emphasize voting as a last resort / fallback.

Steven: please review by March 1st.

### Fraud Prevention Trusted Server

https://docs.google.com/presentation/d/1AtYUKpgSlH3d6ELPhlGcy29IojGk94GZYOXRO78-JtE/view

Phil Lee: work on servers in the Chrome organization. Presentation of collaboration in Google Chrome, Ad @@@, Human Security. Interesting opportunity and whether this group sees potential in this idea or would be a good forum for continuing that work.

Phil: trusted servers for anti-fraud use cases. Timing is good because there are new technology alternatives available today. Server correctness is out of scope for now.

Phil: how do we know what a server does with the data that is sent to it? We want some confidence that what the server tells us it’s doing with the user data is what it’s actually doing with the user data. Who do we want to trust? Author of code for the servers, cloud providers, network intermediaries, etc.

Phil: per earlier discussion, there are also non-technical ways to add trust to a server, including contracts, public attestation, regulations.

Phil: different technologies may have different privacy and security tradeoffs. If we figure out our use cases first, we can then map the technologies to our use cases.

Phil: Aggregate Conversion Measurement proposals use different helper servers operated by different entities; if servers don’t collude, can have higher confidence about control of user data.

Phil: fraud detection across sites may be difficult without third-party cookies. If users click the same ads on many different top-level domains. How do we identify those in order to address fraud in that case?

Phil: open questions. What use cases spring to mind with an environment of some servers that can see some amount of user data but limits placed on what they do with it?

Sam: third party fraud prevention companies would run these servers? Or some other operators? Google?
Phil: could see different configurations.
Sam: so there would be some contexts where third-party cookies, entropy, unmasked ip could still be accessed?
Phil: as third-party cookies go away, some use of data but making sure it’s very controlled

Philipp Pf: use case: might care about a malware fingerprint, just need to know whether it’s a match. 
Sam: makes sense for repeated bad actors like botnets or fraud attacks against an institution. Separate from the characteristics needed for identifying a user’s identity for the first time.

Samuel Ménard: gather behavioral data from the user to determine whether it’s a real user clicking on an ad or just a bot.

Chris Wood: does the trusted server also see the context of the user action? Or does it only see the limited information determining if it’s human or not, legitimate or not? Having a trusted server that sees both the “who” and the “what” would be not great. Being considered in IETF Privacy Pass WG considering attestation, those servers don’t see the context of the action.

Phil: Yeah, that is an excellent question. For the usecase, where we need to think about the different data that we need. Need to explore that. Don’t have good answers right now, if we find usecases that only work with that amount of data.

Per: Chris, good question. From the ad-fraud perspective, the desire is to collect all possible information. With Trusted Servers, the idea is that we can’t collect all data as today; but the trusted server may collected similar data like UA string, IP addresses; but you cannot use the data for other non-approved purposes. We want to correlate/combine data from an anonymized user across different sites to find abuse. We need to figure out, if there is enough “trust” in this Trusted Server context to be able to collect such data. Some more sensitive data there than elsewhere.

Vacha: In the usecase that you called out (100 users, clicking on same 100 ads), could you give an idea of how it would be solved, since identity is fragmented across many contexts. 

Per: In that context, you can replace third-party cookie with other psuedonymous identifiers created from other signals like IP address. Therefore you can use a statistical model based on those identifiers.

Sam Jackson: I don’t necessarily that all such data is necessary. It may be possible to do some obfuscation so the identifiers may be useful for abuse detection, but not on an ongoing basis for tracking.

Nick: Not clear how we can expect to prevent abuse on behalf of servers, such as signals being used for tracking. APIs have been abused for constant user surveillance. DO we have a solution in mind, or will we deal with it later:

Phil: Not particularly decided, but some ideas. Browsers can require that the code has to be open-source or use some technology. We’re not just putting the data on a server and hoping that noone abuses it

Nick: That’s already the case right? E.g. I can configure my browser to disable 3p cookies. Do we have something different, or will that continue?

Per: Key point is to figure out how to make sure servers aren’t using the data for other purposes. In the end, will likely need to rely on non-technical mechanisms like audits/policy/compliance. We should start thinking about high level concepts where technical solutions by themselves cannot guarantee.

Sam Jackson: Markup could be used to indicate use, so it can also be surfaced to users.

Chris Wood: Want to follow-up on what Sam said earlier. It may be that not all trusted servers will see the same data. Would be good to cover this in the Google Doc under review.

Phil: We are over time, happy to continue or cut off.

Phil: The next question I had (as someone not familiar with anti-fraud) was about what sorts of signals you need to collect. Assuming it’s not confidential.

Kevin Gibbons: Can partially answer that. We prevent account takeovers, which is mostly about identifying repeated visitors to individual sites (not across sites). We look at user agents, user behavior, platform, UA version, etc. But: this is an adversarial game. We are looking for things to identify when someone is making too many transactions, but eventually attackers can figure out the signals and bypass our measures; so then we have to use other signals; so hard to answer with a list of signals.

Per: IP address, UA strings, etc. Anything that bad actors are less likely to abuse. If we provide a list, then bad actors have a checklist to work. 3ve white paper shows sophistication. We had to build custom signal collection to solve that case. It’s an ongoing arms race to catch. Really important to fall into the trap of there being a silver bullet. We cannot stay ahead of the bad actors if we do that.

Phil: That makes sense Really good to talk about.

Phil: Next thing, what else is involved beyond user data? I’m assuming algorithms are proprietary/confidential. Is that right?

Sam Jackson: That’s right. Machine learning models, etc. that need to be updated on an ongoing basis.

Per: Correct. For example, think about how credit card companies prevent credit card fraud. Have to keep techniques confidential. There is some public information, and others are highly proprietary/confidential. Primarily need to prevent information access to bad actors. Baseline infrastructure can be open source; but not algorithms.

Chris Wood: There seems to be tension between keeping the heuristic confidential, and asking clients to share with trusted servers. Client may not feel compelled to share anything, unless client has confidence that servers aren’t abusing the data. Might be worth facroring that into considering whether the algorithms are public or not.

Philipp: Baseline stuff, like looking at distributions can be public; but keep private about things like thresholds. High entropy signals that can’t be used for tracking, but useful for modeling.

Sameer: EMVCo looks at three components (user data, …, …) to analyze payment transaction for fraud detection.

Steven: Quickly go over remaining open questions, and we can discuss on Slack/list. Closing queue.

Phil: Other things - which technical methods matches our usecases and privacy guarantees we are looking for. Human access may be necessary for debugging, as well as for finding new trends and attacks, since this is an adversarial problem. Who owns the servers. Tons of interesting things to talk about.

Steven: Hopefully we’ll get further discussion offline.

Steven: Moving on to use-case proposals by Dimitris.

Dimitris:W3C Anti-Fraud Working Group - Use Cases & Threat Models

Dimitris: Thank you for the contributions so far. Lots of new content around manual attacks, which was not adequately represented in the initial draft. Some usecases about the banking sector. Will do some restructuring and cleaning up the doc. Not sure of exact timelines; but is it reasonable to call for adoption in 2-3 weeks?

Steven: Once a doc gets adopted, we can expect it to change over time, so we can keep adding uecases even after that.

Dimitris: Does the group have any questions on the doc?

Per: Adversarial space, so use cases evolve as attackers find new ways. Should be careful not to call it a final list of use cases.

Steven: Noone on queue; but folks can continue to look offline.

Dimitris: Will keep a close eye and accept suggestions.

Steven: Bring it back into a GitHub version once doc is stable, for version control purposes.

Steven: Reviewing proposals. Phil’s TRusted Servers proposal. 
Safe lists proposal by Sam

Sam JAckson: Proposal builds off presentation from two weeks. Two aspects of this proposal that tries to address questions that came up with the Trusted Servers proposal. Markup to indicate that data isn’t ubiquitously collected, but only when the user needs to prove authenticity. Define scope (URL where data is collected), or reference to a iframe on authority’s domain. Criteria for certification - we can come up with requirements in this group or elsewhere. Organization would have to go through auditing to get added to browser safelists. Users would need to agree to provide more data in a high risk context. In broader industry groups, there is some thinking about ad-hoc consent flows that needs to be considered.

The proposal: https://github.com/antifraudcg/proposals/issues/4

Steven: Reminder to file new issues on proposals repo if you have proposals. Please email list as well when you submit proposals. We could also set up GitHub reports from the proposals repo to be sent to the mailing list.

Brad: One issue may be double notifications from GitHub+mailing list. What HTTPWG does might be a compromise.

Steven: Will set that up and go over in the next meeting.

Steven: We have an Anti-Fraud CG channel on W3C Slack; please try joining before the next meeting.

Steven: Next meeting in 2 weeks. Please reach out to chairs, or file an issue in the meeting repository.
